import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Define a preprocessing function
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize the text
    words = word_tokenize(text)
    # Remove stopwords
    words = [word for word in words if word not in stopwords.words('english')]
    return words

# Apply the preprocessing function to the caption column
df['processed_captions'] = df['caption_column'].apply(preprocess_text)

# Define a list of keywords to search for
keywords = ['Palestine', 'Gaza', 'Genocide Joe']

# Function to check for keywords in text
def contains_keywords(text):
    return any(keyword in text for keyword in keywords)

# Apply the function to the processed captions
df['contains_keywords'] = df['processed_captions'].apply(contains_keywords)

# Filter the rows that contain the keywords
filtered_df = df[df['contains_keywords']]
print(filtered_df)

# Count the frequency of each keyword
from collections import Counter

# Flatten the list of processed captions
all_words = [word for words in df['processed_captions'] for word in words]

# Count the frequency of keywords
keyword_counts = Counter([word for word in all_words if word in keywords])

# Display the keyword counts
print(keyword_counts)

